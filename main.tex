\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
%\usepackage{algorithmic}
\usepackage[]{algorithm2e} 
\usepackage{subcaption}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{booktabs}
\usepackage{xcolor}
%\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

%\setlength\textfloatsep{5pt}  % separation of floats on bottom or top of page
%\setlength\intextsep{5pt}  % separation of floats inbetween text

\title{Inverse Sizing and Exploration Based on Diffusion Models and Structural Knowledge
\thanks{Parts of this work have been done within the collaborative project HoLoDEC funded by the Federal Ministry of Education and Research Germany (BMBF) under the funding code 16ME0705.}}

%\author{\IEEEauthorblockN{Markus Leibl}
%\IEEEauthorblockA{\textit{Chair of Electronic Design Automation} \\
%\textit{Technical University of Munich}\\
%Munich, Germany \\
%markus.leibl@tum.de}
%\and
%\IEEEauthorblockN{Helmut Graeb}
%\IEEEauthorblockA{\textit{Chair of Electronic Design Automation} \\
%\textit{Technical University of Munich}\\
%Munich, Germany \\
%helmut.graeb@tum.de}
%}


\maketitle

\begin{abstract}
Quick and effortless exploration of design spaces is of major importance in the face of rapid development cycles, varying specifications and increasingly complex circuits.
It is also well known, that the exploitation of slack in specifications by making adjustments to transistor device sizes can lead to improved yield.
We address those challenges by combining two state of the art machine learning approaches to achieve accurate generation of optimal performance points, while also allowing for the possibility to explore nearby sizings that might lead to more robust designs. To this end, a diffusion model is combined with an algorithm, that analyzes the circuit and splits the problem into simpler subproblems. The models are tested on a set of typical operational amplifiers.
\end{abstract}

%\begin{IEEEkeywords}
%component, formatting, style, styling, insert
%\end{IEEEkeywords}

\section{Introduction}\label{sec:intro}
	Since the development of SPICE, the industry accepted method for designing analog circuits has not changed much. While the SPICE models keep becoming more complex \cite{gatermann22mosfet}, the approach of manual device tuning persists \cite{gielen23workshop}. Key challenges of making analog CAD tools useful in practice are robustness, ease of use and applicability to a high range of use cases. All those aspects are not easy to grasp on a fundamental level. On the other hand, machine learning approaches excel in modeling nonlinear black box phenomena. Their combination with already existing conventional techniques has a high potential to bridge remaining gaps, that prevent the adoption of tools in industry.
	
	Analog design is inherently an inverse problem. While the calculation of performances, yield etc., is satisfactorily possible via simulation, the inverse, i.e. getting a circuit with optimal parametrization requires experience, trial and error and complex numerical routines. As shown in previous works \cite{ leibl24inverse, lourenco19pareto}, machine learning (ML) methods can be used to accurately reproduce sizings on (pareto) optimal fronts. Furthermore, it has been shown \cite{eid24diffusion} that diffusion models are well adaptable to the inverse sizing problem. Other ML based works attempted to solve this same problem. In \cite{lourenco18}, artificial neural networks (ANNs) are used to guess circuit sizings, utilizing only one ANN for modeling an entire netlist, leading to a higher learning task complexity and thus large sample sizes (in \cite{lourenco18} $16600$ or roughly $16-160 \, x$ sample sizes used in this work) are required to learn the inverse problem within a smaller performance range. In \cite{beaulieu23cascaded}, the problem is broken down into smaller tasks, and one ANN is trained to sequentially size a single device, in the same vein as \cite{leibl24inverse}, feeding the outputs of the previous ANNs to the input of the next, producing a cascade of networks that can be sorted in different ways, until an optimal order is found. The drawback is that for more complex circuits of e.g. 15 devices, optimally ordering the ANNs for sizing leads to a complexity of 15!.
	
	Other works follow the traditional approach of tackling the direct sizing problem, treating it as an optimization problem \cite{fayazi23angel, budak23apostle, wolfe03nnmodel, hakhamaneshi23pretraining}. Reinforcement learning was also studied, with \cite{gao23rose, settaluri22reinforcement, zhang23multiagentRL}  replacing the optimizer with an artificial neural network (ANN), leading to improvements in prediction speed, accuracy, and scalability. 
	However, to the best of our knowledge, none of them show strengths in all four categories, speed, accuracy, sample efficiency and exploration.
	%In the end, all of these works suffer from lengthy training and simulation times.
	
	In this work, we aim for a combination of all those virtues by applying denoising diffusion probabilistic models (DDPM) to the sizing of OpAmps, utilizing the approach in \cite{leibl24inverse} of subdividing the circuits into simpler structures, which has shown high accuracy for low sample sizes. DDPMs have proven their potential for generating strongly varied, but realistic results in other fields, OpenAi's DALL-E and Stable Diffusion being among the most prominent examples. \cite{eid24diffusion} proved these model's efficacy in the inverse sizing problem.
	
%	In this work, we explore the application of denoising diffusion probabilistic models (DDPM) to the sizing of OpAmps, utilizing the approach in \cite{leibl24inverse} of subdividing the circuits into simpler structures, and by leveraging sampled sizings from the open-source tool FUBOCO \cite{fuboco-github}. These models efficacy in the inverse sizing problem were proven in \cite{eid24diffusion}. 
%	Each DDPM is trained on a different structure of the target circuit, learning the corresponding distribution of sizes and respective performances by the noising and denoising process. 
%	After training, the DDPMs can be sampled to generate sizing values for each structure, which when combined make up the whole sizing solution of the target OpAmp. 
	Unlike most machine learning models, DDPMs can suggest different sizings for the same problem by repeatedly sampling them.
	As shown in Section IV, this approach to the inverse problem can produce a more varied set of possible sizings, which is very helpful when exploring sizing options in the vicinity of performance optima. 
%	The contributions of this work are as follows: 
This work contributes a machine learning approach that:
%	\begin{itemize}
%	\item suggest valid sizings that perform in the vicinity of the required metrics, allowing for exploration of a sizing area of interest
%	\item generate sizing solutions at push-button speed
%	\end{itemize}
	\begin{itemize}
	\item can suggest valid sizings with resulting performance in the vicinity of the required metrics, allowing for exploration of sizings while still fulfilling specifications
	\item is highly data efficient
	\item has an accurate expectation of the generated sizings
	\item generates sizing solutions at push-button speed
	\end{itemize}
	
 	In section \ref{sec:intro2}, we provide an overview of the proposed method and its connection to structural analysis. 
 	In sec.~\ref{sec:sizing}, we describe the sizing process in detail. We conclude with a presentation of results for two OpAmps.
	
%
%    With increasing MOSFET model complexity \cite{gatermann22mosfet}, analog design, particularly OpAmp design, remains manual in industrial practice \cite{gielen23workshop}, despite the existence of automatic tools. Various procedural methods for automated design have been discussed in \cite{prautsch16reuse, crossley13bag, schweikardt22scheible}. The idea of using smaller building blocks and hierarchical strategies is successfully demonstrated, e.g., in \cite{meissner15feats, abel2022fuboco, mendhurwar2012structureneural, synthesis23zhang, canelas22hierachical}. 
%	Early on, modeling  performance characteristics of circuits has been popular, e.g. by symbolic analysis \cite{cuatle10symbolic, wambacq95symbolic}. % or machine learning \cite{doboli03piecewise}.
%	Some prior works in the machine learning realm follow the idea of skipping or replacing the optimization algorithm by other means such as neural networks (NNs). In~\cite{lourenco18}, artificial neural networks (ANNs) are used to guess circuit sizings; due to modeling the entire netlist with one ANN and including non-optimal results in the former case, the complexity of the learning task is higher and thus large sample sizes (in \cite{lourenco18} $16600$ or roughly $16-160 \, x$ our sample sizes) are required to learn the inverse problem within a smaller performance range. Similarly to our work,  \cite{lourenco19pareto} uses optimal samples for training but without exploiting structural knowledge. In~\cite{gao23rose, settaluri22reinforcement}, reinforcement  algorithms are presented, substituting the optimizer with an NN.
%%	 while maintaining an iterative nature. 
%	 Here, the simulation effort is increased as multiple rounds are required for every sizing, similar to traditional optimization. In~\cite{beaulieu23cascaded}, the complexity of the NNs is reduced by having small networks represent single device parameters and cascading them in different ways, feeding outputs of earlier networks to the input of later networks in the tree.
%	 Unfortunately, with a circuit of e.g. $15$ devices/networks, optimally ordering them in a chain can lead to a complexity of $15!$ in the worst case.
%	Other works, such as \cite{fayazi23angel, budak23apostle, wolfe03nnmodel, hakhamaneshi23pretraining}, follow the traditional approach of treating sizing as an optimization problem or concentrate on performance modeling. While achieving good accuracy in their findings, they contend with the costly iteration process inherent in optimization.
%\newline
%	In this paper, utilizing sampled sizings from the open-source tool FUBOCO \cite{fuboco-github}, we introduce a machine learning algorithm for sizing operational amplifiers \textbf{without the need for optimization and simulation} during the deployment phase. 
%	\newline
%	Our approach stands out by \textbf{capitalizing on automatically extracted structural} and \textbf{functional information}, a departure from related works. This brings about benefits in terms of speed, flexibility, and accuracy. The required sample size, setup- and evaluation-time, as well as necessary expert knowledge are kept at a minimum. Additional contributions include the creation of a comprehensive database of netlists, the development of a custom loss function, and the utilization of a quasi-Newton optimizer for training purposes.
%\newline
%	 In section \ref{sec:intro2}, we provide an overview of the proposed method and its connection to structural analysis. 
%	 In sec.~\ref{sec:sizing}, we describe the sizing process in detail. We conclude with a presentation of results for two OpAmps.
	 
	 	
 	\section{From Structural Analysis to Circuit Sizing via Machine Learning}\label{sec:intro2}
	\begin{figure}[h]
		\centering
        \includegraphics[width=\linewidth]{figures/struct_overview_incl_partition}
		\setlength{\abovecaptionskip}{0ex}%
		\setlength{\belowcaptionskip}{-2ex}%
		\caption{Splitting various OpAmps into canonical building blocks with different functional characteristics and accumulating intrinsic design know-how by training each building block with random optimal samples from all available OpAmps. Indices {\color{red} $b$}, {\color{blue} $l$} and {\color{green} $t$} represent {\color{red}$bias$}, {\color{blue}$load$} and {\color{green}$transconductance$} functionality of devices. Every building block maps the overall OpAmp performance to its sizing.}
		\label{fig:overview-structure}
	\end{figure}
	\setlength{\abovecaptionskip}{1ex}%
	\setlength{\belowcaptionskip}{-3ex}%
 	\subsection{Basics of Structural and Functional Blocks}
 	Our method heavily relies on decomposing circuits into subcircuits, which we call structural building blocks. Most building blocks are known to the analog designer by names such as \textit{current mirror} or \textit{differential pair}.

	We further distinguish different functional variants of one and the same structural block.
	To this end, we assign to every transistor in a circuit its functional property, out of three different possibilities: \textit{bias, load} or \textit{transconductance}. 
	 For example, a \textit{simple current mirror} (scm) can act as \textit{load}, but in other parts of a circuit, the same building block can act as \textit{bias}. And sometimes, some part of a current mirror can act as \textit{load}, while another part acts as \textit{bias}. Figure~\ref{fig:overview-structure} shows all structural blocks and their functional variants from our dataset.
	An example of decomposing a Folded-Cascode OpAmp into blocks can be seen in figure~\ref{fig:simple53}. Its structural building blocks are shaded, the names annotated close by.
	Structural and functional block recognition is fully automated. The time required for this is much less than one second. 
	
\subsection{Overview of the Method}

	\begin{figure}[]
		\begin{subfigure}{.48\textwidth}
			\centering
			\includegraphics[width=\linewidth]{figures/simple52_partitioning}
			\setlength{\abovecaptionskip}{0ex}%
	%		\setlength{\belowcaptionskip}{-2ex}%
			\setlength{\belowcaptionskip}{0ex}%
			\caption{Folded Cascode Operational Amplifier (OpAmp A).}
			\label{fig:simple53}
		\end{subfigure}
			\hfill
		\begin{subfigure}{.5\textwidth}
			\centering
			\includegraphics[width=\linewidth]{figures/finetuning-networks2}
			\setlength{\abovecaptionskip}{-2ex}%
	%		\setlength{\belowcaptionskip}{-2ex}%
			\caption{Process of transfer learning.}
			\label{fig:fine-tuning}
		\end{subfigure}
			\setlength{\abovecaptionskip}{3ex}%
			\setlength{\belowcaptionskip}{-3ex}%
%			\caption{Folded Cascode OpAmp (\ref{fig:simple53}) and process of transfer learning (\ref{fig:fine-tuning}): Instead of training one huge network per OpAmp, we can use weights from our database as initial training points to merely fine-tune our networks }
			\caption{Instead of training one huge network per OpAmp, we can split an Opamp, e.g. OpAmp A, into blocks (shading and coloring in fig.~\ref{fig:simple53}) and use weights from our network-database as initial training points to merely fine-tune our networks~(\ref{fig:fine-tuning}). }
			\label{fig:training-process}
		\end{figure}
		
		\begin{figure}[]
		\begin{subfigure}{0.12\textwidth}
		\resizebox{!}{\textwidth}{\includegraphics[]{figures/perf53/gain}}
		\vspace{-1.4\baselineskip}
		\caption{{\footnotesize gain ($dB$)}}
		\label{fig:53gain}
		\vspace{1.3\baselineskip}
		\end{subfigure}
		\hfill
		\begin{subfigure}{0.12\textwidth}
		\resizebox{!}{\textwidth}{\includegraphics[]{figures/perf53/ft}}
		\vspace{-1.4\baselineskip}
		\caption{{\footnotesize trns. fr. ($Mhz$)}}
		\label{fig:53ft}
		\vspace{1.3\baselineskip}
		\end{subfigure}
		\hfill
		\begin{subfigure}{0.12\textwidth}
		\resizebox{!}{\textwidth}{\includegraphics[]{figures/perf53/slew}}
		\vspace{-1.4\baselineskip}
		\caption{{\footnotesize slew rate ($\frac{V}{\mu s}$)}}
		\label{fig:53slew}
		\vspace{1.3\baselineskip}
		\end{subfigure}
		\hfill
		\begin{subfigure}{0.12\textwidth}
		\resizebox{!}{\textwidth}{\includegraphics[]{figures/perf53/power}}
		\vspace{-1.4\baselineskip}
		\caption{{\footnotesize power ($mW$)}}
		\label{fig:53pow}
	%	\vspace{.4\baselineskip}
		\end{subfigure}
		\hfill
		\begin{subfigure}{0.12\textwidth}
		\resizebox{!}{\textwidth}{\includegraphics[]{figures/perf53/phase}}
		\vspace{-1.4\baselineskip}
		\caption{{\footnotesize phase margin}}
		\label{fig:53ph}
	%	\vspace{.4\baselineskip}
		\end{subfigure}
		\hfill
		\begin{subfigure}{0.12\textwidth}
		\resizebox{!}{\textwidth}{\includegraphics[]{figures/perf53/cmrr}}
		\vspace{-1.4\baselineskip}
		\caption{{\footnotesize CMRR}}
		\label{fig:53cmrr}
	%	\vspace{.4\baselineskip}
		\end{subfigure}
		\setlength{\abovecaptionskip}{4ex}%
		\setlength{\belowcaptionskip}{-4ex}%
		\caption{Histograms of some performance features for the Folded-Cascode OpAmp in Fig.~\ref{fig:simple53}.}
		\label{fig:perffeatures53}
		\end{figure}
	
\section{DDPM Background and Implementation}\label{sec:ddpm}
	As mentioned in sec \ref{sec:intro}, several DDPMs are trained to learn the sizing and respective performances distribution of the different structures that make up the opamp. DDPMs were first introduced in \cite{ho2020denoisingdiffusionprobabilisticmodels} as a new diffusion model parametrization, which are a class of NNs mostly used in an imaging context, like data generation and restauration. These models are composed of 3 different processes, which are briefly highlighted here:
	
	\subsubsection{Forward Process}
	First, the training data $x_{t}$ is systematically destroyed by the addition of noise over $T$ sequential timesteps. This noise is sampled from a Gaussian distribution and at the end of the $T$ timesteps, the resulting training data distribution resembles itself a Gaussian distribution. This process is demonstrated by equation \ref{eq:forward}, where $\beta_{t}$ is the variance scheduler, a hyperparameter that controls how much noise is added to the data $x$ at each time timestep $t$.
	
	\begin{equation} \label{eq:forward}
	q(x_{t}|x_{t-1}) := \mathcal{N}(x_{t};\mu,\Sigma) := \mathcal{N}(x_{t};\sqrt{1-\beta_{t}}x_{t-1}, \beta_{t}I)
	\end{equation}
	
	\subsubsection{Reverse Process}
	After the data is destroyed at $x_{T}$, an ANN is trained to try to reverse the forward process, either by predicting the original data $x_{0}$ up front, or by predicting another value that can be used to reconstruct the original data, like the added noise, etc. This process is represented by equation \ref{eq:reverse}, where $\alpha_{t}=1-\beta_{t}$ and $\overline{\alpha}_{t}=\prod_{s=1}^{t}\alpha_{s}$.
	
	\begin{equation} \label{eq:reverse}
		\rho_{\theta}(x_{t-1}|x_{t}) := \mathcal{N}(x_{t-1};\mu_{\theta}(x_{t},t),\Sigma_{\theta}(x_{t},t)) := \mathcal{N}(x_{t};\frac{1}{\sqrt{\alpha_{t}}}(x_{t}-\frac{1-\alpha_{t}}{\sqrt{1-\overline{\alpha}_{t}}}\epsilon_{\theta}(x_{t},t)),\beta_{t}I) 
	\end{equation}
	
	The correlation between the forward and reverse process to noise and denoise images can be seen in Fig \ref{fig:forward_reverse}.
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=\linewidth]{figures/ForwardReverseDDPM}
		\setlength{\abovecaptionskip}{0ex}%
		\setlength{\belowcaptionskip}{-2ex}%
		\caption{Correlation between forward and reverse}
		\label{fig:forward_reverse}
	\end{figure}
	
	\subsubsection{Sampling Process}
	Finally, after the model is trained, the sampling process is used to generate new data. The model learned to reconstruct the original data, and so by giving it pure Gaussian noise it tries to reconstruct data. Except this time, there is no actual data in the first place, marking this a form of generative AI.
	
	It was shown by \cite{eid24diffusion} that DDPMs produce interesting results when applied to the inverse sizing problem. In this work, a similar implementation is considered, with a cosine schedule for $\beta$ and classifier-free guidance \cite{ho2022classifierfreediffusionguidance}, but with 2 key differences:
	
	\begin{itemize}
		\item instead of predicting the noise $\epsilon$, the implemented model predicts a velocity equation $v$ which has been shown to enable a true signal-to-noise ratio of $0$ at the last timestep $T$, removing an important discrepancy between training and inference \cite{lin2024commondiffusionnoiseschedules};
		\item the backbone of the model implemented here is a transformer, with an architecture similar to \cite{peebles2023scalablediffusionmodelstransformers}, as opposed to the simple multi-layer perceptrons implemented by \cite{eid24diffusion}. To predict a more complex value in $v$ a stronger architecture was required.
	\end{itemize}
	
	
\section{Conclusion}
	

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,bibtex-base-iee}

\end{document}

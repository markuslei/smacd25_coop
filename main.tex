\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
%\usepackage{algorithmic}
\usepackage[]{algorithm2e} 
\usepackage{subcaption}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{booktabs}
\usepackage{xcolor}
%\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

%\setlength\textfloatsep{5pt}  % separation of floats on bottom or top of page
%\setlength\intextsep{5pt}  % separation of floats inbetween text

\title{Inverse Sizing and Exploration Based on Diffusion Models and Structural Knowledge
\thanks{Parts of this work have been done within the collaborative project HoLoDEC funded by the Federal Ministry of Education and Research Germany (BMBF) under the funding code 16ME0705.}}

%\author{\IEEEauthorblockN{Markus Leibl}
%\IEEEauthorblockA{\textit{Chair of Electronic Design Automation} \\
%\textit{Technical University of Munich}\\
%Munich, Germany \\
%markus.leibl@tum.de}
%\and
%\IEEEauthorblockN{Helmut Graeb}
%\IEEEauthorblockA{\textit{Chair of Electronic Design Automation} \\
%\textit{Technical University of Munich}\\
%Munich, Germany \\
%helmut.graeb@tum.de}
%}


\maketitle

\begin{abstract}
Quick and effortless exploration of design spaces is of major importance in the face of rapid development cycles, varying specifications and increasingly complex circuits.
It is also well known, that the exploitation of slack in specifications by making adjustments to transistor device sizes can lead to improved yield.
We address those challenges by combining two state of the art machine learning approaches to achieve accurate generation of optimal performance points, while also allowing for the possibility to explore nearby sizings that might lead to more robust designs. To this end, a diffusion model is combined with an algorithm, that analyzes the circuit and splits the problem into simpler subproblems. The models are tested on a set of typical operational amplifiers.
\end{abstract}

%\begin{IEEEkeywords}
%component, formatting, style, styling, insert
%\end{IEEEkeywords}

\section{Introduction}\label{sec:intro}
	Since the development of SPICE, the industry accepted method for designing analog circuits has not changed much. While the SPICE models keep becoming more complex \cite{gatermann22mosfet}, the approach of manual device tuning persists \cite{gielen23workshop}. Key challenges of making analog CAD tools useful in practice are robustness, ease of use and applicability to a high range of use cases. All those aspects are not easy to grasp on a fundamental level. On the other hand, machine learning approaches excel in modeling nonlinear black box phenomena. Their combination with already existing conventional techniques has a high potential to bridge remaining gaps, that prevent the adoption of tools in industry.
	
	Analog design is inherently an inverse problem. While the calculation of performances, yield etc., is satisfactorily possible via simulation, the inverse, i.e. getting a circuit with optimal parametrization requires experience, trial and error and complex numerical routines. As shown in previous works \cite{ leibl24inverse, lourenco19pareto}, machine learning (ML) methods can be used to accurately reproduce sizings on (pareto) optimal fronts. Furthermore, it has been shown \cite{eid24diffusion} that diffusion models are well adaptable to the inverse sizing problem. Other ML based works attempted to solve this same problem. In \cite{lourenco18}, artificial neural networks (ANNs) are used to guess circuit sizings, utilizing only one ANN for modeling an entire netlist, leading to a higher learning task complexity and thus large sample sizes (in \cite{lourenco18} $16600$ or roughly $16-160 \, x$ sample sizes used in this work) are required to learn the inverse problem within a smaller performance range. In \cite{beaulieu23cascaded}, the problem is broken down into smaller tasks, and one ANN is trained to sequentially size a single device, in the same vein as \cite{leibl24inverse}, feeding the outputs of the previous ANNs to the input of the next, producing a cascade of networks that can be sorted in different ways, until an optimal order is found. The drawback is that for more complex circuits of e.g. 15 devices, optimally ordering the ANNs for sizing leads to a complexity of 15!.
	
	Other works follow the traditional approach of tackling the direct sizing problem, treating it as an optimization problem \cite{fayazi23angel, budak23apostle, wolfe03nnmodel, hakhamaneshi23pretraining}. Reinforcement learning was also studied, with \cite{gao23rose, settaluri22reinforcement, zhang23multiagentRL}  replacing the optimizer with an artificial neural network (ANN), leading to improvements in prediction speed, accuracy, and scalability. 
	However, to the best of our knowledge, none of them show strengths in all four categories, speed, accuracy, sample efficiency and exploration.
	%In the end, all of these works suffer from lengthy training and simulation times.
	
	In this work, we aim for a combination of all those virtues by applying denoising diffusion probabilistic models (DDPM) to the sizing of OpAmps, utilizing the approach in \cite{leibl24inverse} of subdividing the circuits into simpler structures, which has shown high accuracy for low sample sizes. DDPMs have proven their potential for generating strongly varied, but realistic results in other fields, OpenAi's DALL-E and Stable Diffusion being among the most prominent examples. \cite{eid24diffusion} proved these model's efficacy in the inverse sizing problem.
	
%	In this work, we explore the application of denoising diffusion probabilistic models (DDPM) to the sizing of OpAmps, utilizing the approach in \cite{leibl24inverse} of subdividing the circuits into simpler structures, and by leveraging sampled sizings from the open-source tool FUBOCO \cite{fuboco-github}. These models efficacy in the inverse sizing problem were proven in \cite{eid24diffusion}. 
%	Each DDPM is trained on a different structure of the target circuit, learning the corresponding distribution of sizes and respective performances by the noising and denoising process. 
%	After training, the DDPMs can be sampled to generate sizing values for each structure, which when combined make up the whole sizing solution of the target OpAmp. 
	Unlike most machine learning models, DDPMs can suggest different sizings for the same problem by repeatedly sampling them.
	As shown in Section IV, this approach to the inverse problem can produce a more varied set of possible sizings, which is very helpful when exploring sizing options in the vicinity of performance optima. 
%	The contributions of this work are as follows: 
This work contributes a machine learning approach that:
%	\begin{itemize}
%	\item suggest valid sizings that perform in the vicinity of the required metrics, allowing for exploration of a sizing area of interest
%	\item generate sizing solutions at push-button speed
%	\end{itemize}
	\begin{itemize}
	\item can suggest valid sizings with resulting performance in the vicinity of the required metrics, allowing for exploration of sizings while still fulfilling specifications
	\item is highly data efficient
	\item has an accurate expectation of the generated sizings
	\item generates sizing solutions at push-button speed
	\end{itemize}
	
 	In section \ref{sec:intro2}, we provide an overview of the proposed method and its connection to structural analysis, as well as describing the sizing process in detail. In section \ref{sec:ddpm}, a short background of DDPMs and our implemented models are presented. We conclude with a presentation of results for two OpAmps, in section \ref{sec:results}.
	
%
%    With increasing MOSFET model complexity \cite{gatermann22mosfet}, analog design, particularly OpAmp design, remains manual in industrial practice \cite{gielen23workshop}, despite the existence of automatic tools. Various procedural methods for automated design have been discussed in \cite{prautsch16reuse, crossley13bag, schweikardt22scheible}. The idea of using smaller building blocks and hierarchical strategies is successfully demonstrated, e.g., in \cite{meissner15feats, abel2022fuboco, mendhurwar2012structureneural, synthesis23zhang, canelas22hierachical}. 
%	Early on, modeling  performance characteristics of circuits has been popular, e.g. by symbolic analysis \cite{cuatle10symbolic, wambacq95symbolic}. % or machine learning \cite{doboli03piecewise}.
%	Some prior works in the machine learning realm follow the idea of skipping or replacing the optimization algorithm by other means such as neural networks (NNs). In~\cite{lourenco18}, artificial neural networks (ANNs) are used to guess circuit sizings; due to modeling the entire netlist with one ANN and including non-optimal results in the former case, the complexity of the learning task is higher and thus large sample sizes (in \cite{lourenco18} $16600$ or roughly $16-160 \, x$ our sample sizes) are required to learn the inverse problem within a smaller performance range. Similarly to our work,  \cite{lourenco19pareto} uses optimal samples for training but without exploiting structural knowledge. In~\cite{gao23rose, settaluri22reinforcement}, reinforcement  algorithms are presented, substituting the optimizer with an NN.
%%	 while maintaining an iterative nature. 
%	 Here, the simulation effort is increased as multiple rounds are required for every sizing, similar to traditional optimization. In~\cite{beaulieu23cascaded}, the complexity of the NNs is reduced by having small networks represent single device parameters and cascading them in different ways, feeding outputs of earlier networks to the input of later networks in the tree.
%	 Unfortunately, with a circuit of e.g. $15$ devices/networks, optimally ordering them in a chain can lead to a complexity of $15!$ in the worst case.
%	Other works, such as \cite{fayazi23angel, budak23apostle, wolfe03nnmodel, hakhamaneshi23pretraining}, follow the traditional approach of treating sizing as an optimization problem or concentrate on performance modeling. While achieving good accuracy in their findings, they contend with the costly iteration process inherent in optimization.
%\newline
%	In this paper, utilizing sampled sizings from the open-source tool FUBOCO \cite{fuboco-github}, we introduce a machine learning algorithm for sizing operational amplifiers \textbf{without the need for optimization and simulation} during the deployment phase. 
%	\newline
%	Our approach stands out by \textbf{capitalizing on automatically extracted structural} and \textbf{functional information}, a departure from related works. This brings about benefits in terms of speed, flexibility, and accuracy. The required sample size, setup- and evaluation-time, as well as necessary expert knowledge are kept at a minimum. Additional contributions include the creation of a comprehensive database of netlists, the development of a custom loss function, and the utilization of a quasi-Newton optimizer for training purposes.
%\newline
%	 In section \ref{sec:intro2}, we provide an overview of the proposed method and its connection to structural analysis. 
%	 In sec.~\ref{sec:sizing}, we describe the sizing process in detail. We conclude with a presentation of results for two OpAmps.
	 
	 	
 	\section{From Structural Analysis to Circuit Sizing via Machine Learning}\label{sec:intro2}
	\begin{figure}[h]
		\centering
        \includegraphics[width=\linewidth]{figures/struct_overview_incl_partition}
		\setlength{\abovecaptionskip}{0ex}%
		\setlength{\belowcaptionskip}{-2ex}%
		\caption{Splitting various OpAmps into canonical building blocks with different functional characteristics and accumulating intrinsic design know-how by training each building block with random optimal samples from all available OpAmps. Indices {\color{red} $b$}, {\color{blue} $l$} and {\color{green} $t$} represent {\color{red}$bias$}, {\color{blue}$load$} and {\color{green}$transconductance$} functionality of devices. Every building block maps the overall OpAmp performance to its sizing.}
		\label{fig:overview-structure}
	\end{figure}
	\setlength{\abovecaptionskip}{1ex}%
	\setlength{\belowcaptionskip}{-3ex}%
 	\subsection{Basics of Structural and Functional Blocks}
 	As detailed in \cite{ leibl24inverse}, the method  relies on the decomposition of circuits into subcircuits, which we call structural building blocks.
 	While the structure of the blocks is defined by their topology, i.e. the devices and their interconnections, we identify three functional properties, \textit{bias, load} or \textit{transconductance}, for each individual transistor. This leads to further discrimination among the structural blocks. But because not every combination of functional properties is useful, the total set of possible structures remains small. 
	All possible variants of blocks can be found in figure~\ref{fig:overview-structure}.
	
	An example of decomposing a Folded-Cascode OpAmp into blocks can be seen in figure~\ref{fig:simple53}. Its structural building blocks are shaded, the names annotated close by.
	Structural and functional block recognition is fully automated. The time required for this is much less than one second. 
	
\subsection{Overview of the Method}
	\begin{figure}[]
		\centering
		\includegraphics[width=\linewidth]{figures/simple52_partitioning}
%		\setlength{\abovecaptionskip}{0ex}%
%		\setlength{\belowcaptionskip}{-2ex}%
		\caption{Folded-Cascode OpAmp. The circuit is decomposed into its structural and functional blocks. Here the structural blocks are shaded, while the functionality of the devices is represented by colours. }
		\label{fig:simple53}
	\end{figure}
	
	In contrast to \cite{ leibl24inverse} we use a simplified version of the model. We skip the pretraining step and directly finetune the networks. Furthermore, instead of using gain boosting for sizing of $L$, we employ the same model type.
	The sizing process is then straight forward:
	\begin{itemize}
	\item We split the opamp into the predefined subblocks.
	\item We train a neural network for each of the subblocks, where we use the OpAmp performance as guidance for the DDPM model and train to recover only the sizings of the blocks.
	\item Finally, at evaluation time, we recombine the predicted sizings. In case of overlap of two blocks, we take the average value.
	\end{itemize}
	Like in the previous work, we enforce symmetry that can be automatically detected by deterministic algorithms. For symmetry within blocks, this is done during training, for symmetry outside of blocks, this is done during recombination.
			
%	\begin{figure}[]
%		\begin{subfigure}{.48\textwidth}
%			\centering
%			\includegraphics[width=\linewidth]{figures/simple52_partitioning}
%			\setlength{\abovecaptionskip}{0ex}%
%	%		\setlength{\belowcaptionskip}{-2ex}%
%			\setlength{\belowcaptionskip}{0ex}%
%			\caption{Folded Cascode Operational Amplifier (OpAmp A).}
%			\label{fig:simple53}
%		\end{subfigure}
%			\hfill
%		\begin{subfigure}{.5\textwidth}
%			\centering
%			\includegraphics[width=\linewidth]{figures/finetuning-networks2}
%			\setlength{\abovecaptionskip}{-2ex}%
%	%		\setlength{\belowcaptionskip}{-2ex}%
%			\caption{Process of transfer learning.}
%			\label{fig:fine-tuning}
%		\end{subfigure}
%			\setlength{\abovecaptionskip}{3ex}%
%			\setlength{\belowcaptionskip}{-3ex}%
%%			\caption{Folded Cascode OpAmp (\ref{fig:simple53}) and process of transfer learning (\ref{fig:fine-tuning}): Instead of training one huge network per OpAmp, we can use weights from our database as initial training points to merely fine-tune our networks }
%			\caption{Instead of training one huge network per OpAmp, we can split an Opamp, e.g. OpAmp A, into blocks (shading and coloring in fig.~\ref{fig:simple53}) and use weights from our network-database as initial training points to merely fine-tune our networks~(\ref{fig:fine-tuning}). }
%			\label{fig:training-process}
%		\end{figure}
		
		\begin{figure}[]
		\begin{subfigure}{0.12\textwidth}
		\resizebox{!}{\textwidth}{\includegraphics[]{figures/perf53/gain}}
		\vspace{-1.4\baselineskip}
		\caption{{\footnotesize gain ($dB$)}}
		\label{fig:53gain}
		\vspace{1.3\baselineskip}
		\end{subfigure}
		\hfill
		\begin{subfigure}{0.12\textwidth}
		\resizebox{!}{\textwidth}{\includegraphics[]{figures/perf53/ft}}
		\vspace{-1.4\baselineskip}
		\caption{{\footnotesize trns. fr. ($Mhz$)}}
		\label{fig:53ft}
		\vspace{1.3\baselineskip}
		\end{subfigure}
		\hfill
		\begin{subfigure}{0.12\textwidth}
		\resizebox{!}{\textwidth}{\includegraphics[]{figures/perf53/slew}}
		\vspace{-1.4\baselineskip}
		\caption{{\footnotesize slew rate ($\frac{V}{\mu s}$)}}
		\label{fig:53slew}
		\vspace{1.3\baselineskip}
		\end{subfigure}
		\hfill
		\begin{subfigure}{0.12\textwidth}
		\resizebox{!}{\textwidth}{\includegraphics[]{figures/perf53/power}}
		\vspace{-1.4\baselineskip}
		\caption{{\footnotesize power ($mW$)}}
		\label{fig:53pow}
	%	\vspace{.4\baselineskip}
		\end{subfigure}
		\hfill
		\begin{subfigure}{0.12\textwidth}
		\resizebox{!}{\textwidth}{\includegraphics[]{figures/perf53/phase}}
		\vspace{-1.4\baselineskip}
		\caption{{\footnotesize phase margin}}
		\label{fig:53ph}
	%	\vspace{.4\baselineskip}
		\end{subfigure}
		\hfill
		\begin{subfigure}{0.12\textwidth}
		\resizebox{!}{\textwidth}{\includegraphics[]{figures/perf53/cmrr}}
		\vspace{-1.4\baselineskip}
		\caption{{\footnotesize CMRR}}
		\label{fig:53cmrr}
	%	\vspace{.4\baselineskip}
		\end{subfigure}
		\setlength{\abovecaptionskip}{4ex}%
		\setlength{\belowcaptionskip}{-4ex}%
		\caption{Histograms of some performance features for the Folded-Cascode OpAmp in Fig.~\ref{fig:simple53}.}
		\label{fig:perffeatures53}
		\end{figure}
	
\section{DDPM Background and Implementation}\label{sec:ddpm}
	As mentioned in the previous sections, several DDPMs are trained to learn the sizing and respective performances distribution of the different structures that make up an OpAmp. DDPMs were first introduced in \cite{ho2020denoisingdiffusionprobabilisticmodels} as a new diffusion model parametrization, which are a class of NNs mostly used in an imaging context, like data generation and restauration. These models are composed of 3 different processes, which are briefly highlighted here:
	
	\subsubsection{Forward Process}
	First, the training data $x_{0}$ is systematically destroyed by the addition of noise over $T$ sequential timesteps, until a terminal distribution $x_{T}$ is reached. This noise is sampled from a Gaussian distribution and at the end of the $T$ timesteps, the resulting training data distribution resembles itself a Gaussian distribution. This process is demonstrated by equation \ref{eq:forward}, where $\beta_{t}$ is the variance scheduler, a hyperparameter that controls how much noise is added to the data $x$ at each time timestep $t$.
	
	\begin{equation} \label{eq:forward}
	\begin{aligned}
	q(x_{t}|x_{t-1}) &:= \mathcal{N}(x_{t};\mu,\Sigma) \\ &:= \mathcal{N}(x_{t};\sqrt{1-\beta_{t}}x_{t-1}, \beta_{t}I)
	\end{aligned}
	\end{equation}
	
	\subsubsection{Reverse Process}
	After the data is destroyed until $x_{t}$, an ANN is trained to try to reverse the forward process, either by predicting the original data $x_{0}$ up front, or by predicting another value that can be used to reconstruct the original data, like the added noise, etc. This process is represented by equation \ref{eq:reverse}, where $\alpha_{t}=1-\beta_{t}$ and $\overline{\alpha}_{t}=\prod_{s=1}^{t}\alpha_{s}$.
	
	\begin{equation} \label{eq:reverse}
	\begin{split}
		\rho_{\theta}(x_{t-1}|x_{t}) := \mathcal{N}(x_{t-1};\mu_{\theta}(x_{t},t),\Sigma_{\theta}(x_{t},t)) \\:= \mathcal{N}(x_{t};\frac{1}{\sqrt{\alpha_{t}}}(x_{t}-\frac{1-\alpha_{t}}{\sqrt{1-\overline{\alpha}_{t}}}\epsilon_{\theta}(x_{t},t)),\beta_{t}I) 
	\end{split}
	\end{equation}
	
	The correlation between the forward and reverse process to noise and denoise images can be seen in figure \ref{fig:forward_reverse}. An overview of the training phase for the Folded-Cascode OpAmp of figure \ref{fig:simple53} can be seen in figure \ref{fig:DDPMtraining}.
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=\linewidth]{figures/ForwardReverseDDPM}
		\setlength{\abovecaptionskip}{0ex}%
		\setlength{\belowcaptionskip}{-2ex}%
		\caption{Correlation between forward and reverse processes.}
		\label{fig:forward_reverse}
	\end{figure}
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=\linewidth]{figures/DDPMSubBlockTraining}
		\setlength{\abovecaptionskip}{0ex}%
		\setlength{\belowcaptionskip}{-2ex}%
		\caption{Training phase for sizing an OpAmp. After the decomposition into sub-blocks, noise is added to the respective sizings. A DDPM is trained for each sub-block to predict the original sizing values.}
		\label{fig:DDPMtraining}
	\end{figure}
	
	\subsubsection{Sampling Process}
	Finally, after the model is trained, the sampling process is used to generate new data. The model learned to reverse the forward process, and so, when giving it pure Gaussian noise, it tries to reverse the noise addition until arriving at the original data. Except this time, there is no actual data to begin with, marking this a form of generative AI. This stage is similar to the training phase shown in figure \ref{fig:DDPMtraining}, except random noise is given to the network instead of the noisy sizings.
	
	It was shown by \cite{eid24diffusion} that DDPMs produce interesting results when applied to the inverse sizing problem. In this work, a similar implementation is considered, with a cosine schedule for $\beta$ and classifier-free guidance \cite{ho2022classifierfreediffusionguidance}, but with 2 key differences:
	
	\begin{itemize}
		\item instead of predicting the noise $\epsilon$, we implement a model that predicts a velocity equation $v=\alpha_{t}\epsilon - \sigma_{t}x$ which has been shown to enable a true signal-to-noise ratio of $0$ at the last timestep $T$ (meaning that at the last timstep the input of the model is pure Gaussian noise), as opposed to other DDPM parametrizations. This removes an important discrepancy between training and inference \cite{lin2024commondiffusionnoiseschedules};
		\item the backbone of the model implemented  is a transformer, with an architecture similar to the "adaLN-Zero" model of  \cite{peebles2023scalablediffusionmodelstransformers}, as opposed to the simple multi-layer perceptrons implemented by \cite{eid24diffusion}. To predict a more complex value in $v$, a stronger architecture was required.
	\end{itemize}
	
	
	
	\section{Experimental Results} \label{sec:results}
	\begin{figure}[]
		\centering
		\includegraphics[width=.7\linewidth]{figures/miller_pmos}
%		\setlength{\abovecaptionskip}{0ex}%
%		\setlength{\belowcaptionskip}{-2ex}%
		\caption{Miller OpAmp}
		\label{fig:miller}
	\end{figure}
	
	
\section{Conclusion}
	

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,bibtex-base-iee}

\end{document}
